from selenium import webdriver
from selenium.webdriver.common.by import By

import time
from datetime import date
import json
import csv

# 设置所需抓取页数
pages = 3
# 输出文件数据
result_data = []
fieldnames = ['title', 'edb_id', 'authors', 'created']
# 抓取信息列表
titles = []
edb_id_list = []
authors = []
dates = []
# code_list = []

# 创建Chrome WebDriver对象
driver = webdriver.Chrome()


def parse_url_list(target, pages):
    url_list = []

    # 目标动态网页的URL
    driver.get(target)

    for i in range(pages):
        # 延时，等待渲染
        time.sleep(3)

        # 提取详情页链接
        links = driver.find_elements(By.XPATH, "//tbody/tr[@role='row']/td[5]/a")
        for url in links:
            url_list.append(url.get_attribute('href'))

        # 注意!!!向下滚动才能加载出NEXT按钮
        mid_url = driver.find_element(By.XPATH, "//tbody/tr[@role='row'][7]")
        driver.execute_script("arguments[0].scrollIntoView();", mid_url)

        # 下一页
        time.sleep(3)
        next_button = driver.find_element(By.XPATH, "//li[@class='paginate_button page-item next']/a")
        next_button.click()

        # 注意还要向上滚动
        time.sleep(3)
        above = driver.find_element(By.XPATH, "//tbody/tr[@role='row'][12]/td[5]/a")
        driver.execute_script("arguments[0].scrollIntoView();", above)

    return url_list


def parse_inf(url_list):
    # 进入详情页进一步爬取
    for url in url_list:
        driver.get(url)

        title = driver.find_element(By.XPATH, "//h1").text
        edb_id = driver.find_element(By.XPATH, "//div[@class='col-sm-12 col-md-6 col-lg-3 d-flex align-items-stretch']["
                                               "1]//div[@class='col-6 text-center'][1]/h6").text
        date = driver.find_element(By.XPATH, "//div[@class='col-sm-12 col-md-6 col-lg-3 d-flex align-items-stretch']["
                                             "3]//div[@class='col-6 text-center'][2]/h6").text
        author = driver.find_element(By.XPATH, "//div[@class='col-sm-12 col-md-6 col-lg-3 d-flex align-items-stretch']["
                                               "2]//div[@class='col-6 text-center'][1]/h6").text
        driver.execute_script("window.scrollBy(0,500);")
        # code = driver.find_elements(By.XPATH, "//code[@class='language-txt']")

        titles.append(title)
        edb_id_list.append(edb_id)
        authors.append(author)
        dates.append(date)
        # code_list.append(code)

        data = {
            'title': title,
            'edb_id': edb_id,
            'authors': author,
            'created': date,
            # 'code': code,
        }

        result_data.append(data)


spider_time = date.today()
urls = parse_url_list("https://www.exploit-db.com/", pages)
parse_inf(urls)

# 验证结果
# print(urls)
# print(len(urls))
# print(titles)
# print(edb_id_list)
# print(authors)
# print(dates)
# print(len(result_data))

# 生成JSON文件
with open(f'./result/{spider_time.strftime("%Y_%m_%d")}爬取前{pages}页.json', 'w', encoding='utf-8') as json_file:
    json.dump(result_data, json_file, ensure_ascii=False, indent=4)

# 生成CSV文件
with open(f'./result/{spider_time.strftime("%Y_%m_%d")}爬取前{pages}页.csv', 'w', encoding='utf-8') as csv_file:
    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
    # 写入表头
    writer.writeheader()
    # 写入数据行
    for data in result_data:
        writer.writerow(data)

# 结束关闭WebDriver
driver.quit()
